# 5회차 스터디

# CNN

**FNN:** 인접 픽셀간의 상관관계가 무시되어 이미지를 벡터화하는 과정에서 정보손실 발생

필터를 이용하여 feature map을 얻음.

Convolution Layer: 입력 이미지에 다양한 필터를 적용하여 특징을 추출함.

### 활성화 함수

선형 변환 만으로는 복잡한 함수를 표현할 수 없음 → 비선형적인 활성화 함수가 필요함.

→ 선형 함수만으로는 여러 층을 쌓아도 하나의 선형 변환으로 축소됨.

따라서 비선형 활성화 함수를 통해 복잡/추상적인 특징을 학습할 수 있음.

### Batch Normalization

정규화가 없다면? → 큰 스케일의 특성이 과도한 영향을 미칠 수 있음.

학습 속도 향상(빠르게 수렴 가능)

센터값을 0으로 가져옴으로 정규화(Normalization)를 할 수 있음.

- 전체 데이터셋이 아닌, 현재의 미니 배치 내에 있는 데이터에만 집중함. (병렬로 처리가 가능, 일반적으로 32~256개의 데이터가 `1batch size`)

데이터를 전처리함. 배치들을 정규분포식처럼 변경시키는 느낌.

- 언제 적용해야 할까?
  - 배치 단위의 학습 → 계층 별로 입력의 데이터 분포가 달라질 수 있음. (Internal Covariant Shift)
  - 블록마다 하나씩 넣어줘서 ReLU와 함께 연산함.
  - 배치 단위 간에 데이터 분포의 차이가 발생함.

# Transfer Learning

높은 수준의 데이터는 얻기 어려움. → 데이터가 비쌈.

데이터가 많이 없을 때 사용할 수 있는 방법

→ **이미 학습 시켰던 작업들의 가중치를 가져와서 적용시키는 방법** (학습했던 파라미터를 통해 다른 태스크를 진행함.)

- 데이터가 정말 없을 때
  - Convolutional Layer + Dense Layers 일부를 떼어내서 그래도 적용함 + 일부 새로운 Dense Layer
- 데이터가 조금은 있을 때
  - Convolution Layers에는 learning rate를 적게 주고,
  - Fully Connected Layers에는 learning rate를 높게 줌.

### LoRA (Low-Rank Adaptation)

GPT같은 LLM 모델에서 사용되는 것처럼 큰 머신러닝 모델 → 많은 리소스가 필요함 → **효율적인 파인튜닝 방법**

> 모델이 복잡하더라도 실제로 중요한 정보를 포함하고 있는 차원은 상대적으로 낮음.

행렬곱에서 폭이 작은 걸 제공해서 사이즈 자체를 줄여버림 → 원래 가지고 있던 가중치와 합쳐서 새로운 가중치로 업데이트 함.

# Knowledge Distillation (지식 전이)

우리가 사용할 수 있는 모델의 사이즈는 작음. 작은 모델로 괜찮은 성능을 낼 때 사용함.

- Teacher 네트워크로 얻은 데이터 → Student 네트워크에게 전달함.

우리가 갖고 있는 데이터(하드 타겟, 실제 정답) → 소프트한 확률값을 통해 다른 클래스와의 학습을 유도함.

기존에는 사진을 보고 정답을 딱 매겨 라벨링했지만, **다른 정답과의 확률 값**도 전달하여 학습할 수 있도록 함.
