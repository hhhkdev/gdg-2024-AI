# 4회차 스터디

# Backpropagation and Neural Network

계산 그래프로 Backpropagation 이해하기: 오차 역전파법

> 순전파에서 loss를 구해서 역전파를 어떻게 진행할지

ANN을 학습 시키기 위한 알고리즘 중 하나.

: 내가 뽑아내고자 하는 target과 실제 output을 비교하여 오차값을 뒤로 전파하여 갱신하는 방식.

### 왜 Backpropagation을 사용하는가? **(결론: 복잡해서)**

- 가중치들을 이루는 값 → 생각보다 어려움. 다변량 함수 → loss 값 구하기 어려움.
- 퍼셉트론 여러 겹(layer가 많은 Loss 함수) → 미분값을 구하기 어려움.(많음) → 계산 코스트가 늘어남.
- 신경망은 여러 층으로 이루어짐. (수천수백개) → Loss를 구성함.

Backpropagation 없이 구하는 방법: **수치적 미분**

n개의 가중치가 있다면 손실 함수의 출력(순전파)을 n번 계산해야 함.

입력 데이터가 신경망을 통해 출력으로 변환되어 손실 함수 값 계산(순전파 1번)

손실 함수의 기울기: 각 가중치에 대해 1번의 역전파로 계산함. (**미분 연쇄 법칙**)

### **loss 값을 바탕으로 출발 지점에서 어떤 영향을 받았는지를 확인하는 방법**

뒤로 미분하면 어떤 영향을 주었는지 알 수 있음.

- 덧셈 노드의 역전파: 각각의 노드에게 대해 미분한 값을 그대로 전달해주는 것과 같음.(곱하는 미분값이 어차피 1임)
- 곱셈 노드의 역전파: 그대로 미분한 값을 서로 바꿔 곱함 ($xy$의 경우는 미분할 경우 서로 위치를 바꿔서 곱한 것과 같음)

- ReLU 계층 → 미분값을 구할 때 범위를 나눠야 함.
- $x$가 0보다 클 때는 1이고, 작을 때는 0을 미분 값으로 가짐.

결국은 Gradient가 output에서 출발하여 어떻게 전달되는지를 확인해야 할 듯.

loss를 0으로 줄이기 위하여 파라미터를 조정함.(Optimization 기법) → learning rate 만큼 작은 단위로 변화시킴.

### 파라미터 $x$값이 오르면 최종 Loss에 어떠한 영향을 주는가

데이터 입력 → 순전파 → 손실 계산 → 역전파 → 가중치 최적화 ⇒ 반복
