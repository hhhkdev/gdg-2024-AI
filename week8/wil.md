# 7회차 스터디

# Attention

(활용할 수 있는 분야가 많으므로 잘 복습하는걸로)

Attention: 입력 데이터의 서로 다른 부분들 간의 관련성 계산하는 매커니즘. (자연어 처리에서 생겨난 개념)

→ 입력 데이터 중 일부의 효과를 증강, 다른 일부를 감소 → 중요한 데이터에 집중하자!

- 왜 필요했는가?
  - `seq2seq` 모델 → 단어들의 시퀀스 단위를 입력하여 단어들의 시퀀스 단위가 나옴.
    - 번역/자연어 질문에서 쓰임.
  - 기존에는 입력이 순차적으로 처리함 → 앞에서의 정보가 약해짐.
    (마지막 Hidden State Vector가 디코더의 처음으로 들어가서 다음 단어를 예측함.) - 길어지면 길어질 수록 앞에 있는 단어의 영향력이 약해짐.
  - 입력 문장이 길면 번역 품질이 떨어짐.

⇒ 어텐션으로 해당 문제를 해결함.

### seq2seq with Attention

디코더에서 출력 단어를 예측하는 시점 → 인코더의 전체 입력 문장을 참고함.

모두 동일한 비율이 아닌, **현재 시점에서 연관 있는 단어**에 집중해서 보게 함.

- 디코더: 다음 단어를 예측하기 위한 `hidden state vector`를 생성함. → 인코더의 `hsv`와의 유사성에 따라 유사성이 높으면 Attention Score가 높아짐. → concat

Attention(Q, K, V): Attention Value

주어진 쿼리에 대해서 모든 키와의 유사도를 각각 가짐. → 쿼리랑 키랑 내적해서 구한 스코어 값 → 밸류 값을 가지고 와서 벡테로 만들어줌.

### Mnlti-head Attention [Vision Transformer]

자연어처리 → attention score → 다음을 예측하기 위해 조금더 집중해야 할 부분.

→ self-attention으로 시쿼스의 각 위치가 어느 위치에 집중시키는지 시각화.

사후적으로 모델이 올바로 판단했는지를 맞출 수 있음.

`XAI` → 입력을 모델에 넣었더니 출력이 제대로 나옴. → 모델에서 일어나는 일을 알 수 없으므로 시각화를 하자!

### Attention의 활용

→ ViT(비전 트랜스포머): 비전에서 어텐션을 사용하기 까지 조금 오래 걸림.

Image Classification: 다른 애들보다 더 높은 성능을 찍음.(SoTA) → 단순한 모델들보다 훨씬 많은 데이터 셋을 필요로 함.

# Vision Transformer

이미지들을 patch 단위로 자름. → 단어를 넣은 것처럼 하나씩 입력을 하여 적용함.

seq2seq와 다르게, Attention Score와 다르게, 나와 관련 있는 것들을 비순차적으로 검사함. → 어텐션을 해야 하는 부분을 가져와서 Multi-Head로 들어감…

⇒ 클래스 수만큼의 확률 분표로 정답이 나오게 됨.

위치 정보를 임베딩 데이터처럼 전달했음.

### CNN vs ViT

CNN: 피쳐맵을 여러번 뽑아서 진행함. 컨볼루션 필터 → **지역 특징 추출**. (인접 영역끼리 계층 전달)

- 컨볼루션 연산에 내재된 위치 정보 계산

ViT: 패치 단위로 분할 후 **`Self-Attention`** (모든 패치간 직접적인 정보 교환)
→ 주변 패치와의 관계 파악을 위해 포지션 임베딩 데이터를 추가해줌.

모든 패치의 Attention Score를 전달할 수 있음.

**`CNN`**: 컨볼루션 연산 → 이미지의 Inductive Bias → 처음 보는 데이터를 정확하게 연산하기 위함. (선입견이 큼)

**`ViT`**: Inductive Bias를 효과적으로 활용하지 못함. → 이미지를 패치단위로 자르기 때문에 지역적 특성에 대한 가정이 사라짐. → 포지션 임베딩을 고려하지만 어디있던 간에 달라지지 않음.(몇번째 패치와의 관계냐에 집중)

⇒ Inductive Bias가 강하다고 좋은 것은 아님. 강하면 오히려 방해물이 좋을수도.
